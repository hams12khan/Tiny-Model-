# Tiny-Model-
This project demonstrates **knowledge distillation**, a technique to compress a large deep learning model (teacher) into a smaller, faster model (student), using the MNIST dataset.
